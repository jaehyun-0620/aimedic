import os
import pandas as pd
import tensorflow as tf
from keras.applications import ResNet50, ResNet101, ResNet152, EfficientNetB1
from keras.layers import Conv2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.applications import ResNet50, DenseNet121, MobileNetV2, VGG16
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D

# 설정
base_image_dir = '../project1/train-image/image'
original_csv_dir = './original_datasets'
client_data_dir = './client_datasets'
image_size = (128, 128)
batch_size = 32
epochs = 5
global_epochs = 5
model_save_dir = './saved_models'
target_count = 1000

# 데이터 균형화 함수
def balance_classes(df):
    benign_df = df[df['target'] == 0]
    malignant_df = df[df['target'] == 1]

    if benign_df.empty or malignant_df.empty:
        print("Error: One of the classes has no data.")
        return pd.DataFrame()

    benign_df = benign_df.sample(n=min(len(benign_df), target_count), random_state=42)
    malignant_df = malignant_df.sample(
        n=min(len(malignant_df), target_count),
        replace=len(malignant_df) < target_count,
        random_state=42
    )

    balanced_df = pd.concat([benign_df, malignant_df])
    balanced_df['target'] = balanced_df['target'].astype(str)
    return balanced_df.sample(frac=1).reset_index(drop=True)

# 데이터 준비
def prepare_client_datasets():
    csv_files = [f for f in os.listdir(original_csv_dir) if f.endswith('.csv')]
    os.makedirs(client_data_dir, exist_ok=True)

    for csv_file in csv_files:
        print(f"Processing {csv_file}...")
        df = pd.read_csv(os.path.join(original_csv_dir, csv_file), low_memory=False)
        df['image_path'] = df['isic_id'].apply(lambda x: os.path.join(base_image_dir, f"{x}.jpg"))
        balanced_df = balance_classes(df)

        if balanced_df.empty:
            print(f"Skipping {csv_file} due to insufficient data.")
            continue

        train_df, temp_df = train_test_split(balanced_df, test_size=0.4, random_state=42)
        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

        client_dir = os.path.join(client_data_dir, os.path.splitext(csv_file)[0])
        os.makedirs(client_dir, exist_ok=True)
        train_df.to_csv(os.path.join(client_dir, 'train.csv'), index=False)
        val_df.to_csv(os.path.join(client_dir, 'val.csv'), index=False)
        test_df.to_csv(os.path.join(client_dir, 'test.csv'), index=False)
        print(f"Saved datasets for {csv_file} in {client_dir}.")

# 데이터 로드 및 생성기
def load_data_generator(client_dir):
    image_size = (224, 224) if model_type == "VGG" else (128, 128)
    train_df = pd.read_csv(os.path.join(client_dir, 'train.csv'))
    val_df = pd.read_csv(os.path.join(client_dir, 'val.csv'))
    test_df = pd.read_csv(os.path.join(client_dir, 'test.csv'))

    # target 열을 문자열로 변환
    train_df['target'] = train_df['target'].astype(str)
    val_df['target'] = val_df['target'].astype(str)
    test_df['target'] = test_df['target'].astype(str)

    # 디버깅: target 열 확인
    print("Unique train target values:", train_df['target'].unique())
    print("Unique val target values:", val_df['target'].unique())
    print("Unique test target values:", test_df['target'].unique())

    train_datagen = ImageDataGenerator(
        rescale=1.0 / 255.0,
        rotation_range=15,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )
    test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)

    train_generator = train_datagen.flow_from_dataframe(
        train_df, x_col='image_path', y_col='target', target_size=image_size, batch_size=batch_size, class_mode='binary'
    )
    val_generator = test_datagen.flow_from_dataframe(
        val_df, x_col='image_path', y_col='target', target_size=image_size, batch_size=batch_size, class_mode='binary'
    )
    test_generator = test_datagen.flow_from_dataframe(
        test_df, x_col='image_path', y_col='target', target_size=image_size, batch_size=batch_size, class_mode='binary'
    )

    return train_generator, val_generator, test_generator

# 모델 생성
def create_model(model_type):
    if model_type == "SimpleCNN":
        # 간단한 CNN 모델
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
            MaxPooling2D(pool_size=(2, 2)),
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D(pool_size=(2, 2)),
            Conv2D(128, (3, 3), activation='relu'),
            Flatten(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(1, activation='sigmoid')
        ])
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

    elif model_type == "efficientNet":
        base_model = EfficientNetB1(include_top=False, weights=None, input_shape=(128, 128, 3))
    elif model_type == "ResNet":
        base_model = ResNet50(include_top=False, weights=None, input_shape=(128, 128, 3))
    elif model_type == "DenseNet":
        base_model = DenseNet121(include_top=False, weights=None, input_shape=(128, 128, 3))
    elif model_type == "ResNet101":
        base_model = ResNet101(include_top=False, weights=None, input_shape=(128, 128, 3))
    elif model_type == "VGG":
        base_model = VGG16(include_top=False, weights="imagenet", input_shape=(224, 224, 3))
        model = Sequential([
            base_model,
            GlobalAveragePooling2D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(1, activation='sigmoid')
        ])
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # 공통 출력층 추가
    model = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# 연합 학습
def federated_learning(client_dirs, model_type, aggregation_method="mean"):
    global_model = create_model(model_type)
    os.makedirs(model_save_dir, exist_ok=True)
    model_save_path = os.path.join(model_save_dir, f"{model_type}_{aggregation_method}_best_model")

    input_shape = (224, 224, 3) if model_type == "VGG" else (128, 128, 3)
    dummy_input = tf.random.normal([1, *input_shape])  # 가중치 생성에 사용
    global_model(dummy_input)  # 가중치 생성

    best_global_accuracy = 0.0
    best_global_weights = None

    for epoch in range(global_epochs):
        print(f"Global Epoch {epoch + 1}/{global_epochs} for {model_type} with {aggregation_method} aggregation")
        client_weights = []
        client_sample_sizes = []

        for client_id, client_dir in enumerate(client_dirs):
            print(f"Training on Client {client_id + 1} for {model_type}...")
            train_gen, val_gen, test_gen = load_data_generator(client_dir)

            steps_per_epoch = np.ceil(train_gen.samples / batch_size).astype(int)
            validation_steps = np.ceil(val_gen.samples / batch_size).astype(int)

            client_model = create_model(model_type)
            client_model(dummy_input)  # 가중치 초기화
            client_model.set_weights(global_model.get_weights())

            client_model.fit(
                train_gen,
                validation_data=val_gen,
                epochs=epochs,
                steps_per_epoch=steps_per_epoch,
                validation_steps=validation_steps,
                verbose=1
            )
            client_weights.append(client_model.get_weights())
            client_sample_sizes.append(train_gen.samples)

        # 가중치 업데이트: 선택한 aggregation_method 적용
        if aggregation_method == "mean":
            new_weights = [np.mean(w, axis=0) for w in zip(*client_weights)]
        elif aggregation_method == "fedavg":
            total_samples = sum(client_sample_sizes)
            weighted_weights = [
                np.sum([w[i] * (size / total_samples) for w, size in zip(client_weights, client_sample_sizes)], axis=0)
                for i in range(len(client_weights[0]))
            ]
            new_weights = weighted_weights
        elif aggregation_method == "maxweight":
            new_weights = [np.max(w, axis=0) for w in zip(*client_weights)]
        else:
            raise ValueError(f"Unknown aggregation method: {aggregation_method}")

        global_model.set_weights(new_weights)

        # 테스트 데이터로 전역 모델 평가
        global_accuracy = []
        for client_dir in client_dirs:
            _, _, test_gen = load_data_generator(client_dir)
            test_steps = np.ceil(test_gen.samples / batch_size).astype(int)
            _, acc = global_model.evaluate(test_gen, steps=test_steps, verbose=0)
            global_accuracy.append(acc)
        avg_acc = np.mean(global_accuracy)
        print(f"Global Accuracy for {model_type} after Epoch {epoch + 1}: {avg_acc:.4f}")

        if avg_acc > best_global_accuracy:
            best_global_accuracy = avg_acc
            best_global_weights = global_model.get_weights()
            print(f"New best model for {model_type} with accuracy: {best_global_accuracy:.4f}")

    if best_global_weights is not None:
        global_model.set_weights(best_global_weights)
        global_model.save(model_save_path, save_format='tf')  # SavedModel 포맷 사용
        print(f"Best global model for {model_type} saved at {model_save_path}")


if __name__ == "__main__":


    #prepare_client_datasets() #데이터 나누는 작업 처음 실행한다면 실행해야하는 함수

    client_dirs = [os.path.join(client_data_dir, d) for d in os.listdir(client_data_dir) if
                   os.path.isdir(os.path.join(client_data_dir, d))]


    model_types = ["SimpleCNN","ResNet","DenseNet","VGG"]
    aggregation_methods = ["mean", "fedavg", "maxweight"]

    for model_type in model_types:
        for method in aggregation_methods:
            print(f"\nStarting federated learning for model: {model_type} with aggregation method: {method}")
            federated_learning(client_dirs, model_type, aggregation_method=method)









            
